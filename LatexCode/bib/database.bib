% Encoding: UTF-8

@article{ref1,
  author={McAulay, R. and Malpass, M.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, title={Speech enhancement using a soft-decision noise suppression filter}, year={1980},
  volume={28},
  number={2},
  pages={137-145},
  doi={10.1109/TASSP.1980.1163394}
}

@article{ref2,
  title={语音增强技术研究综述},
  author={曹丽静},
  journal={河北省科学院学报},
  volume={37},
  number={2},
  pages={7},
  year={2020},
}

@article{ref3,
  title={All-pole modeling of degraded speech},
  author={ Lim, J.  and Oppenheim and A.},
  journal={Acoustics, Speech and Signal Processing, IEEE Transactions on},
  year={1978},
}

@article{ref4,
  title={浅谈语音增强技术的发展},
  author={黎伟勇},
  journal={大观周刊},
  number={15},
  pages={21-22},
  year={2011},
}

@phdthesis{ref5,
  title={语音识别系统关键技术研究},
  author={刘潇},
  school={哈尔滨工程大学},
  year={2006},
}

@article{ref6,
  title={论语音增强技术的研究背景与意义},
  author={王顺利 and 夏长春 and 付嘉铭},
  journal={电脑迷},
  volume={000},
  number={011},
  pages={63},
  year={2014},
}

@article{ref7,
  title={人工神经网络的发展及应用},
  author={毛健 and 赵红东 and 姚婧婧},
  journal={电子设计工程},
  volume={19},
  number={24},
  pages={4},
  year={2011},
}

@article{ref8,
  title={Deep Neural Networks},
  author={ Diener, Lorenz  and  Janke, Matthias  and  Schultz, Tanja },
  pages={1-7},
  year={2018},
}

@article{ref9,
  title={Deep Learning Architectures: A Hierarchy in Convolution Neural Network Technologies},
  author={ Karkra, S.  and  Singh, P.  and  Kaur, K.  and  Sharma, R. },
  year={2021},
}

@book{ref10,
  title={Deep Learning With PyTorch},
  author={ Chaudhary, A.  and  Chouhan, K. S.  and  Gajrani, J.  and  Sharma, B. },
  publisher={Machine Learning and Deep Learning in Real-Time Applications},
  year={2020},
}

@article{ref11,
  title={Neural network architectures: An introduction},
  author={ Dayhoff, J. E. },
  journal={IEEE Computer Society},
  year={1990},
}

@article{ref12,
  title={语音增强算法的研究与实现},
  author={沈锁金 and 刘伟 and 高颖},
  journal={电声技术},
  volume={40},
  number={12},
  pages={4},
  year={2016},
}

@article{ref13,
  title={语音识别综述},
  author={崔文迪 and 黄关维},
  journal={福建电脑},
  number={1},
  pages={2},
  year={2008},
}

@article{ref14,
  title={Speech Recognition under Noisy Environment},
  author={ Nita, T.  and  Minami, S.  and  Nakayama, A.  and  Onogi, T. },
  journal={Ieice Technical Report Speech},
  volume={94},
  year={1994},
}

@article{ref15,
  title={Iterative and sequential Kalman filter-based speech enhancement algorithms},
  author={Gannot and S. and Burshtein and D. and Weinstein and E.},
  journal={Speech \& Audio Processing IEEE Transactions on},
  year={1998},
}

@article{ref16,
  title={A signal subspace approach for speech enhancement},
  author={ Ephraim, Y.  and  Trees, Hl Van },
  journal={IEEE Transactions on Speech and Audio Processing},
  volume={3},
  number={4},
  pages={251-266},
  year={1995},
}

@book{ref17,
  title={语音信号处理.第2版},
  author={韩纪庆 and 张磊 and 郑铁然},
  publisher={语音信号处理.第2版},
  year={2013},
}

@book{ref18,
  title={语音信号数字处理},
  author={杨行峻},
  publisher={语音信号数字处理},
  year={1995},
}

@article{ref19,
  title={Suppression of acoustic noise in speech using spectral subtraction},
  author={ Boll, S. F. },
  journal={Acoustics Speech \& Signal Processing IEEE Transactions on},
  volume={27},
  number={2},
  pages={113-120},
  year={1979},
}

@article{ref20,
  title={一种变步长LMS自适应滤波算法及分析},
  author={高鹰 and 谢胜利},
  journal={电子学报},
  volume={29},
  number={8},
  pages={4},
  year={2001},
}

@inproceedings{ref21,
  title={Artificial bandwidth extension of speech signals using MMSE estimation based on a hidden Markov model},
  author={ Jax, P.  and  Vary, P. },
  booktitle={Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03). 2003 IEEE International Conference on},
  year={2003},
}

@book{ref22,
  title={Learning Deep Architectures for AI},
  author={ Bengio, Yoshua },
  publisher={Now Publishers Inc.},
  pages={1-127},
  year={2009},
}

@article{ref23,
  title={Reducing the Dimensionality of Data with Neural Networks.},
  author={Hinton and  G., E.  and Salakhutdinov and  R., R. },
  journal={Science},
  year={2006},
 abstract={High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ‘autoencoder’ networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
}

@article{ref24,
  title={Long short-term memory.},
  author={Hochreiter and Sepp and Schmidhuber and Jurgen},
  journal={Neural Computation},
  year={1997},
 abstract={Addresses the problem of storing information over extended time intervals by recurrent backpropagation. Introduction of a novel, efficient, gradient-based method called long short-term memory; Results of experiments with artificial data.},
}

@inproceedings{ref25,
  title={Multilayer Perception Learning Control},
  author={ Verley, G.  and  Beauville, Jpad },
  booktitle={International Euro-par Conference on Parallel Processing},
  year={1996},
 abstract={It has been shown that, when used for pattern recognition with supervised learning, a network with one hidden layer tends to the optimal Bayesian classifier provided that three parameters simultaneously tend to certain limiting values: the sample size and the number of cells in the hidden layer must both tend to infinity and some mean error function over the learning sample must tend to its absolute minimum. When at least one of the parameters is constant (in practice the size of the learning sample), then it is no longer justified mathematically to have the other two parameters tend to the values specified above in order to improve the solution. A lot of research has gone into determining the optimal value of the number of cells in the hidden layer. In this paper, we examine, in a more global manner, the joint determination of optimal values of the two free parameters: the number of hidden cells and the mean error. We exhibit an objective factor of problem complexity: the amount of overlap between classes in the representation space. Contrary to what is generally accepted, we show that networks usually regarded as oversized despite a learning phase of limited duration regularly yield better results than smaller networks designed to reach the absolute minimum of the square error during the learning phase. This phenomenon is all the more noticeable that class overlap is high. To control this latter factor, our experiments used an original pattern recognition problem generator, also described in this paper.},
}

@article{ref26,
  title={Spatially pre-processed speech distortion weighted multi-channel Wiener filtering for noise reduction},
  author={ Wouters, Asm },
  journal={Signal Processing},
  year={2004},
 abstract={In this paper, we establish a generalized noise reduction scheme, called the spatially pre-processed speech distortion weighted multi-channel Wiener filter (SP-SDW-MWF), that encompasses the generalized sidelobe canceller (GSC) and a recently developed multichannel Wiener filtering technique (MWF) as extreme cases. In addition, the scheme allows for in-between solutions such as the speech distortion regularized GSC (SDR-GSC). The SDR-GSC adds robustness against signal model errors to the GSC by taking speech distortion explicitly into account in the design criterion of the adaptive stage. Compared to the widely studied GSC with quadratic inequality constraint (QIC-GSC), the SDR-GSC achieves better noise reduction for small model errors, while guaranteeing robustness against large model errors. In addition, the extra filtering of the speech reference signal in the SP-SDW-MWF further improves the performance. In the absence of model errors and for infinite filter lengths, the SP-SDW-MWF corresponds to a cascade of an SDR-GSC with a speech distortion weighted single-channel Wiener filter. In contrast to the SDR-GSC and the QIC-GSC, its performance does not degrade due to microphone mismatch.},
}

@article{ref27,
  title={Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises},
  author={ Chen, J.  and  Wang, Y.  and  Yoho, S. E.  and  Wang, D. L.  and  Healy, E. W. },
  journal={Journal of the Acoustical Society of America},
  volume={139},
  number={5},
  pages={2604},
  year={2016},
 abstract={Supervised speech segregation has been recently shown to improve human speech intelligibility in noise, when trained and tested on similar noises. However, a major challenge involves the ability to generalize to entirely novel noises. Such generalization would enable hearing aid and cochlear implant users to improve speech intelligibility in unknown noisy environments. This challenge is addressed in the current study through large-scale training. Specifically, a deep neural network (DNN) was trained on 10 000 noises to estimate the ideal ratio mask, and then employed to separate sentences from completely new noises (cafeteria and babble) at several signal-to-noise ratios (SNRs). Although the DNN was trained at the fixed SNR of − 2 dB, testing using hearing-impaired listeners demonstrated that speech intelligibility increased substantially following speech segregation using the novel noises and unmatched SNR conditions of 0 dB and 5 dB. Sentence intelligibility benefit was also observed for normal-hearing listeners in most noisy conditions. The results indicate that DNN-based supervised speech segregation with large-scale training is a very promising approach for generalization to new acoustic environments.},
}

@article{ref28,
  title={Long short-term memory for speaker generalization in supervised speech separation},
  author={ Chen, J.  and  Wang, D. L. },
  journal={Journal of the Acoustical Society of America},
  volume={141},
  number={6},
  pages={4705},
  year={2017},
 abstract={Speech separation can be formulated as learning to estimate a time-frequency mask from acoustic features extracted from noisy speech. For supervised speech separation, generalization to unseen noises and unseen speakers is a critical issue. Although deep neural networks (DNNs) have been successful in noise-independent speech separation, DNNs are limited in modeling a large number of speakers. To improve speaker generalization, a separation model based on long short-term memory (LSTM) is proposed, which naturally accounts for temporal dynamics of speech. Systematic evaluation shows that the proposed model substantially outperforms a DNN-based model on unseen speakers and unseen noises in terms of objective speech intelligibility. Analyzing LSTM internal representations reveals that LSTM captures long-term speech contexts. It is also found that the LSTM model is more advantageous for low-latency speech separation and it, without future frames, performs better than the DNN model with future frames. The propo...},
}

@inproceedings{ref29,
  title={A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement},
  author={ Tan, K.  and  Wang, D. L. },
  booktitle={INTERSPEECH},
  year={2018},
 abstract={Many real-world applications of speech enhancement, such as hearing aids and cochlear implants, desire real-time processing, with no or low latency. In this paper, we propose a novel convolutional recurrent network (CRN) to address real-time monaural speech enhancement. We incorporate a convolutional encoder-decoder (CED) and long short-term memory (LSTM) into the CRN architecture, which leads to a causal system that is naturally suitable for real-time processing. Moreover, the proposed model is noise- and speaker-independent, i.e. noise types and speakers can be different between training and test. Our experiments suggest that the CRN leads to consistently better objective intelligibility and perceptual quality than an existing LSTM based model. Moreover, the CRN has much fewer trainable parameters.},
}

@article{ref30,
  title={Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author={Clevert, Djork-Arné and  Unterthiner, T.  and  Hochreiter, S. },
  journal={Computer Science},
  year={2015},
 abstract={We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
}

@inproceedings{ref31,
  title={Deep Sparse Rectifier Neural Networks},
  author={ Glorot, X.  and  BorDeS, A.  and  Bengio, Y. },
  booktitle={Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={315-323},
  year={2011},
 abstract={While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. \% Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. \% Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
}

@inproceedings{ref32,
  title={Dual-Path RNN: Efficient Long Sequence Modeling for Time-Domain Single-Channel Speech Separation},
  author={ Luo, Y.  and  Chen, Z.  and  Yoshioka, T. },
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2020},
 abstract={Recent studies in deep learning-based speech separation have proven the superiority of time-domain approaches to conventional time-frequency-based methods. Unlike the time-frequency domain approaches, the time-domain separation systems often receive input sequences consisting of a huge number of time steps, which introduces challenges for modeling extremely long sequences. Conventional recurrent neural networks (RNNs) are not effective for modeling such long sequences due to optimization difficulties, while one-dimensional convolutional neural networks (1-D CNNs) cannot perform utterance-level sequence modeling when its receptive field is smaller than the sequence length. In this paper, we propose dual-path recurrent neural network (DPRNN), a simple yet effective method for organizing RNN layers in a deep structure to model extremely long sequences. DPRNN splits the long sequential input into smaller chunks and applies intra- and inter-chunk operations iteratively, where the input length can be made proportional to the square root of the original sequence length in each operation. Experiments show that by replacing 1-D CNN with DPRNN and apply sample-level modeling in the time-domain audio separation network (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved with a 20 times smaller model than the previous best system.},
}

@article{ref33,
  title={ICASSP 2021 Deep Noise Suppression Challenge},
  author={ Reddy, Cka  and  Dubey, H.  and  Gopal, V.  and  Cutler, R.  and  Srinivasan, S. },
  year={2020},
 abstract={The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. We recently organized a DNS challenge special session at INTERSPEECH 2020. We open sourced training and test datasets for researchers to train their noise suppression models. We also open sourced a subjective evaluation framework and used the tool to evaluate and pick the final winners. Many researchers from academia and industry made significant contributions to push the field forward. We also learned that as a research community, we still have a long way to go in achieving excellent speech quality in challenging noisy real-time conditions. In this challenge, we are expanding both our training and test datasets. There are two tracks with one focusing on real-time denoising and the other focusing on real-time personalized deep noise suppression. We also make a non-intrusive objective speech quality metric called DNSMOS available for participants to use during their development stages. The final evaluation will be based on subjective tests.},
}

@article{ref34,
  title={Phase-aware Speech Enhancement with Deep Complex U-Net},
  author={ Choi, H. S.  and  Kim, J. H.  and  Huh, J.  and  Kim, A.  and  Ha, J. W.  and  Lee, K. },
  year={2019},
 abstract={Most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. This is due to the difficulty of estimating the phase of clean speech. To improve speech enhancement performance, we tackle the phase estimation problem in three ways. First, we propose Deep Complex U-Net, an advanced U-Net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued spectrograms. Second, we propose a polar coordinate-wise complex-valued masking method to reflect the distribution of complex ideal ratio masks. Third, we define a novel loss function, weighted source-to-distortion ratio (wSDR) loss, which is designed to directly correlate with a quantitative evaluation measure. Our model was evaluated on a mixture of the Voice Bank corpus and DEMAND database, which has been widely used by many deep learning models for speech enhancement. Ablation experiments were conducted on the mixed dataset showing that all three proposed approaches are empirically valid. Experimental results show that the proposed method achieves state-of-the-art performance in all metrics, outperforming previous approaches by a large margin.},
}

@article{ref35,
  title={DCU-net: a deformable convolutional neural network based on cascade U-net for retinal vessel segmentation},
  author={ Yang, X.  and  Li, Z.  and  Guo, Y.  and  Zhou, D. },
  journal={Multimedia Tools and Applications},
  volume={81},
  number={11},
  pages={15593-15607},
  year={2022},
 abstract={To further improve retinal vessel segmentation accuracy, we propose a deformable convolutional neural network based on cascade U-Net for retinal vessel segmentation: DCU-Net. The overall structure of DCU-Net is composed of two U-Net. We introduce deformable convolution to build a feature extraction module, which enhances the modeling ability of the model for vessel deformation. For improving the efficiency of information transfer between U-Net models, we use a residual channel attention module to connect U-Net. DCUNet achieves excellent results on public datasets. On DRIVE and CHASE_DB1 datasets, the Acc reaches 0.9568, 0.9664, respectively, the AUC reaches 0.9810, and 0.9872, respectively. From the experimental results, the residual channel attention module and residual deformable convolution module greatly improve the retinal vessel segmentation accuracy. The comprehensive performance of our method is better than that of some state-of-the-art methods.},
}

@article{ref36,
  title={U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author={ Ronneberger, O.  and  Fischer, P.  and  Brox, T. },
  journal={Springer International Publishing},
  year={2015},
 abstract={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong...},
}

@article{ref37,
  title={DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement},
  author={ Hu, Y.  and  Liu, Y.  and  Lv, S.  and  Xing, M.  and  Xie, L. },
  year={2020},
 abstract={Speech enhancement has benefited from the success of deep learning in terms of intelligibility and perceptual quality. Conventional time-frequency (TF) domain methods focus on predicting TF-masks or speech spectrum, via a naive convolution neural network (CNN) or recurrent neural network (RNN). Some recent studies use complex-valued spectrogram as a training target but train in a real-valued network, predicting the magnitude and phase component or real and imaginary part, respectively. Particularly, convolution recurrent network (CRN) integrates a convolutional encoder-decoder (CED) structure and long short-term memory (LSTM), which has been proven to be helpful for complex targets. In order to train the complex target more effectively, in this paper, we design a new network structure simulating the complex-valued operation, called Deep Complex Convolution Recurrent Network (DCCRN), where both CNN and RNN structures can handle complex-valued operation. The proposed DCCRN models are very competitive over other previous networks, either on objective or subjective metric. With only 3.7M parameters, our DCCRN models submitted to the Interspeech 2020 Deep Noise Suppression (DNS) challenge ranked first for the real-time-track and second for the non-real-time track in terms of Mean Opinion Score (MOS).},
}

@article{ref38,
  title={Deep Complex Networks},
  author={ Trabelsi, C.  and  Bilaniuk, O.  and  Zhang, Y.  and  Serdyuk, D.  and  Subramanian, S.  and  Santos, Joo Felipe  and  Mehri, S.  and  Rostamzadeh, N.  and  Bengio, Y.  and  Pal, C. J. },
  year={2017},
 abstract={At present, the vast majority of machine learning and deep learning methods operate on real-valued data. The data distribution as well as intermediate features are represented as real numbers. Recent work on recurrent neural networks suggests that complex numbers could have a richer representational capacity and could also facilitate noise-robust memory retrieval mechanisms. Residual network architectures for training very deep neural networks have also seen widespread adoption.In this work, we present a general formulation for complex-valued feed-forward architectures and apply it to convolutional residual networks. We also present complex batch-normalization, a weight initialization strategy for complex-valued neural nets and an end-to-end trainable convolutional neural network based on complex convolutions and representations. We demonstrate that such complex-valued models are able to achieve comparable or better performance than their real-valued counterparts.},
}

@article{ref39,
  title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  author={ He, K.  and  Zhang, X.  and  Ren, S.  and  Sun, J. },
  journal={CVPR},
  year={2015},
 abstract={Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.},
}

@article{ref40,
  title={End-to-End Multi-Channel Speech Separation},
  author={ Gu, R.  and  Wu, J.  and  Zhang, S. X.  and  Chen, L.  and Y Xu and  Yu, M.  and  Su, D.  and Y Zou and  Yu, D. },
  year={2019},
 abstract={The end-to-end approach for single-channel speech separation has been studied recently and shown promising results. This paper extended the previous approach and proposed a new end-to-end model for multi-channel speech separation. The primary contributions of this work include 1) an integrated waveform-in waveform-out separation system in a single neural network architecture. 2) We reformulate the traditional short time Fourier transform (STFT) and inter-channel phase difference (IPD) as a function of time-domain convolution with a special kernel. 3) We further relaxed those fixed kernels to be learnable, so that the entire architecture becomes purely data-driven and can be trained from end-to-end. We demonstrate on the WSJ0 far-field speech separation task that, with the benefit of learnable spatial features, our proposed end-to-end multi-channel model significantly improved the performance of previous end-to-end single-channel method and traditional multi-channel methods.},
}

@inproceedings{ref41,
  title={Dual-Path RNN for Long Recording Speech Separation},
  author={ Li, C.  and  Luo, Y.  and  Han, C.  and  Li, J.  and  Chen, Z. },
  booktitle={Spoken Language Technology Workshop},
  year={2021},
}

@article{ref42,
  title={DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement},
  author={ Le, X.  and  Chen, H.  and  Chen, K.  and  Lu, J. },
  year={2021},
 abstract={The dual-path RNN (DPRNN) was proposed to more effectively model extremely long sequences for speech separation in the time domain. By splitting long sequences to smaller chunks and applying intra-chunk and inter-chunk RNNs, the DPRNN reached promising performance in speech separation with a limited model size. In this paper, we combine the DPRNN module with Convolution Recurrent Network (CRN) and design a model called Dual-Path Convolution Recurrent Network (DPCRN) for speech enhancement in the time-frequency domain. We replace the RNNs in the CRN with DPRNN modules, where the intra-chunk RNNs are used to model the spectrum pattern in a single frame and the inter-chunk RNNs are used to model the dependence between consecutive frames. With only 0.8M parameters, the submitted DPCRN model achieves an overall mean opinion score (MOS) of 3.57 in the wide band scenario track of the Interspeech 2021 Deep Noise Suppression (DNS) challenge. Evaluations on some other test sets also show the efficacy of our model.},
}

@article{ref43,
  title={产生标准高斯白噪声序列的方法},
  author={蔡坤宝 and 王成良 and 陈曾汉},
  journal={中国电机工程学报},
  volume={24},
  number={12},
  pages={5},
  year={2004},
 abstract={高斯白色噪声序列在科学研究与工程领域得到广泛的应用,如系统辨识与仿真,电气与通信工程,生物医学工程等.然而,在已有的商业软件包中难以找到确实能产生标准高斯白噪声序列的程序段.该文提出了在计算机上产生标准高斯白噪声序列的新方法,可有效地产生N(0,1)分布并具有良好白色性能的高斯随机序列,其计算方法主要有两个部分组成:首先应用改进的Marsaglia-Bray方法产生标准正态分布的随机序列;然后,在均方误差最小的准则下,应用双随机交换最小化方法对序列进行白化处理.所得序列的幅值分布与给定的理论正态分布相一致,且序列具有优良的白色化随机性能.},
}
